<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Analysis - Neuroscience Fiction</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Feature Analysis</h1>
            <p class="subtitle">Analyze your feature ratings to optimize your brain prediction model</p>
        </header>

        <!-- Progress Indicator -->
        <div class="progress-indicator">
            <div class="progress-step active" data-step="1">
                <div class="step-number">1</div>
                <div class="step-label">Load Data</div>
            </div>
            <div class="progress-step" data-step="2">
                <div class="step-number">2</div>
                <div class="step-label">Reliability</div>
            </div>
            <div class="progress-step" data-step="3">
                <div class="step-number">3</div>
                <div class="step-label">Agreement</div>
            </div>
            <div class="progress-step" data-step="4">
                <div class="step-number">4</div>
                <div class="step-label">Redundancy</div>
            </div>
            <div class="progress-step" data-step="5">
                <div class="step-number">5</div>
                <div class="step-label">Similarity</div>
            </div>
            <div class="progress-step" data-step="6">
                <div class="step-number">6</div>
                <div class="step-label">Decisions</div>
            </div>
        </div>

        <!-- Step 1: Load Data -->
        <section class="step-section active" id="step1">
            <div class="card">
                <div class="step-header">
                    <h2>Step 1: Load Your Ratings Data</h2>
                </div>

                <div class="explanation">
                    <p>Let's start by loading your feature ratings from the server. You'll need your <strong>year</strong> and <strong>group name</strong>.</p>
                    <p class="note">Your ratings may include both human raters and LLM models (surrogate raters like GPT-5 or Gemini).</p>
                </div>

                <div class="input-section">
                    <div class="input-group">
                        <label for="yearInput">Year:</label>
                        <input type="text" id="yearInput" placeholder="e.g., 2025" value="2025">
                    </div>
                    <div class="input-group">
                        <label for="groupNameInput">Group Name:</label>
                        <input type="text" id="groupNameInput" placeholder="e.g., YourGroupName">
                    </div>
                    <button id="loadDataBtn" class="btn-primary">Load Data</button>
                </div>

                <div id="loadingIndicator" class="loading-message" style="display: none;">
                    <div class="spinner"></div>
                    <p>Loading your ratings data...</p>
                </div>

                <div id="dataLoadError" class="error-message" style="display: none;"></div>

                <div id="dataSummary" class="data-summary" style="display: none;">
                    <h3>Data Loaded Successfully!</h3>
                    <div class="summary-grid">
                        <div class="summary-item">
                            <div class="summary-label">Items:</div>
                            <div class="summary-value" id="numItems">0</div>
                        </div>
                        <div class="summary-item">
                            <div class="summary-label">Features:</div>
                            <div class="summary-value" id="numFeatures">0</div>
                        </div>
                        <div class="summary-item">
                            <div class="summary-label">Raters:</div>
                            <div class="summary-value" id="numRaters">0</div>
                        </div>
                        <div class="summary-item">
                            <div class="summary-label">Total Ratings:</div>
                            <div class="summary-value" id="totalRatings">0</div>
                        </div>
                    </div>

                    <div id="droppedRatersSection" class="dropped-raters-section" style="display: none;">
                        <h4>⚠️ Raters Excluded (Incomplete Ratings):</h4>
                        <p class="explanation-text">The following raters were excluded because they didn't complete all ratings:</p>
                        <div id="droppedRatersList" class="dropped-raters-list"></div>
                    </div>

                    <div class="rater-breakdown">
                        <h4>Active Raters:</h4>
                        <div id="raterList" class="rater-list"></div>
                    </div>

                    <details class="data-preview">
                        <summary>Preview Data (first 10 rows)</summary>
                        <div id="dataPreviewTable"></div>
                    </details>
                </div>

                <div class="step-navigation">
                    <button id="step1Next" class="btn-next" disabled>Next: Feature Reliability →</button>
                </div>
            </div>
        </section>

        <!-- Step 2: Feature Reliability -->
        <section class="step-section" id="step2">
            <div class="card">
                <div class="step-header">
                    <h2>Step 2: Feature Reliability</h2>
                </div>

                <div class="explanation">
                    <h3>What is Feature Reliability?</h3>
                    <p><strong>Also known as "Inter-rater Agreement"</strong> in the literature, this measures how consistently different raters (humans and LLMs) agreed on each feature.</p>

                    <details>
                        <summary>How does it work?</summary>
                        <div class="explanation-detail">
                            <p><strong>For each feature separately:</strong></p>
                            <ol>
                                <li>Take all items rated by Rater 1 (e.g., 60 ratings)</li>
                                <li>Take all items rated by Rater 2 (e.g., 60 ratings)</li>
                                <li>Correlate these two 60-element vectors</li>
                                <li>Repeat for all pairs of raters</li>
                                <li>Average these pairwise correlations to get reliability for this feature</li>
                            </ol>
                            <p><strong>Example:</strong> If you have 60 items and 5 raters, for the feature "animacy" we correlate each rater's 60 item ratings with every other rater's 60 item ratings (10 pairs total), then average those 10 correlations.</p>
                            <p><strong>Higher values = better reliability</strong> (raters agreed more on this feature)</p>
                            <p>Features with low reliability (&lt; 0.5) may be subjective or poorly defined.</p>
                        </div>
                    </details>

                    <div class="note">
                        <strong>Note on LLM Raters:</strong> LLM models may show different consistency patterns than humans.
                        Deterministic LLMs (temperature=0) might have perfect self-consistency, while humans have natural variation.
                        Both are valid - focus on overall agreement across all raters.
                    </div>
                </div>

                <div id="reliabilityAnalysis">
                    <button id="computeReliabilityBtn" class="btn-primary">Compute Feature Reliability</button>

                    <div id="reliabilityLoading" class="loading-message" style="display: none;">
                        <div class="spinner"></div>
                        <p>Computing correlations...</p>
                    </div>

                    <div id="reliabilityResults" style="display: none;">
                        <div class="results-header">
                            <h3>Feature Reliability Results</h3>
                            <button id="downloadReliabilityChart" class="btn-download">Download Chart (PNG)</button>
                        </div>

                        <div class="chart-container">
                            <canvas id="reliabilityChart"></canvas>
                        </div>

                        <details class="data-table-section">
                            <summary>View Data Table</summary>
                            <div id="reliabilityTable"></div>
                        </details>

                        <div class="interpretation">
                            <h4>Interpretation Guide:</h4>
                            <ul>
                                <li><strong>r &gt; 0.7:</strong> Excellent reliability - strong agreement</li>
                                <li><strong>r = 0.5-0.7:</strong> Good reliability - moderate agreement</li>
                                <li><strong>r = 0.3-0.5:</strong> Fair reliability - consider reviewing feature definition</li>
                                <li><strong>r &lt; 0.3:</strong> Poor reliability - feature may be too subjective</li>
                                <li><strong>NaN (Not a Number):</strong> No variance - all raters gave identical ratings for this feature across all items. This means the feature doesn't discriminate between items in your dataset.</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-navigation">
                    <button id="step2Prev" class="btn-prev">← Previous</button>
                    <button id="step2Next" class="btn-next" disabled>Next: Rater Agreement →</button>
                </div>
            </div>
        </section>

        <!-- Step 3: Rater Agreement -->
        <section class="step-section" id="step3">
            <div class="card">
                <div class="step-header">
                    <h2>Step 3: Rater Agreement</h2>
                </div>

                <div class="explanation">
                    <h3>What is Rater Agreement?</h3>
                    <p>This analysis identifies which raters consistently agree with the group across all features. Unlike Step 2 which evaluates features, this evaluates individual rater performance.</p>

                    <details>
                        <summary>How does it work?</summary>
                        <div class="explanation-detail">
                            <p><strong>For each feature separately:</strong></p>
                            <ol>
                                <li>Correlate each rater's item ratings (60 items) with every other rater's item ratings</li>
                                <li>For each rater, average their correlations with all other raters</li>
                                <li>This gives each rater an "agreement score" for this feature</li>
                            </ol>
                            <p><strong>Then across all features:</strong></p>
                            <ol start="4">
                                <li>Average each rater's agreement scores across all features</li>
                                <li>This gives each rater an overall agreement score</li>
                            </ol>
                            <p><strong>Example:</strong> If Rater 1 has 0.8 agreement on "animacy", 0.7 on "size", and 0.6 on "color", their overall agreement is (0.8+0.7+0.6)/3 = 0.70</p>
                            <p><strong>Higher values = better agreement</strong> with the group consensus</p>
                            <p>Low-agreement raters may be using different criteria or misunderstanding the task.</p>
                        </div>
                    </details>

                    <div class="note">
                        <strong>Note on LLM vs Human Raters:</strong> LLM models may show different agreement patterns:
                        <ul>
                            <li>LLMs with identical prompts may have near-perfect agreement</li>
                            <li>Different LLM models may disagree systematically</li>
                            <li>Human raters typically show more natural variation</li>
                            <li>Low agreement between humans and LLMs isn't necessarily bad - it may reflect different valid perspectives</li>
                        </ul>
                    </div>
                </div>

                <div id="agreementAnalysis">
                    <button id="computeAgreementBtn" class="btn-primary">Compute Rater Agreement</button>

                    <div id="agreementLoading" class="loading-message" style="display: none;">
                        <div class="spinner"></div>
                        <p>Computing rater-vs-rater correlations...</p>
                    </div>

                    <div id="agreementResults" style="display: none;">
                        <div class="results-header">
                            <h3>Rater Agreement Results</h3>
                            <button id="downloadAgreementChart" class="btn-download">Download Heatmap (PNG)</button>
                        </div>

                        <!-- Average Agreement Table -->
                        <div class="agreement-summary">
                            <h4>Average Agreement by Rater</h4>
                            <p class="explanation-text">This shows how well each rater correlates with all other raters, on average across all features.</p>
                            <div id="agreementSummaryTable"></div>
                        </div>

                        <!-- Heatmap -->
                        <div class="heatmap-section">
                            <h4>Rater-vs-Rater Correlation Heatmap</h4>
                            <p class="explanation-text">
                                Average correlation between each pair of raters across all features.
                                For each feature, we correlate Rater A's 60 item ratings with Rater B's 60 item ratings,
                                then average these correlations across all features.
                            </p>
                            <div id="agreementHeatmap" class="plotly-heatmap"></div>
                        </div>

                        <details class="data-table-section">
                            <summary>View Detailed Data Table</summary>
                            <div id="agreementDetailTable"></div>
                        </details>

                        <div class="interpretation">
                            <h4>Interpretation Guide:</h4>
                            <ul>
                                <li><strong>r &gt; 0.7:</strong> Excellent agreement - rater closely aligns with group</li>
                                <li><strong>r = 0.5-0.7:</strong> Good agreement - reasonable alignment</li>
                                <li><strong>r = 0.3-0.5:</strong> Fair agreement - some divergence from group</li>
                                <li><strong>r &lt; 0.3:</strong> Poor agreement - rater may be using different criteria</li>
                                <li><strong>NaN (zero-variance features):</strong> Features where all raters gave identical ratings for all items. These don't discriminate between items and are automatically excluded from agreement calculations.</li>
                                <li><strong>Consider dropping raters with consistently low agreement (&lt; 0.3)</strong></li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-navigation">
                    <button id="step3Prev" class="btn-prev">← Previous</button>
                    <button id="step3Next" class="btn-next" disabled>Next: Feature Redundancy →</button>
                </div>
            </div>
        </section>

        <!-- Step 4: Feature Redundancy -->
        <section class="step-section" id="step4">
            <div class="card">
                <div class="step-header">
                    <h2>Step 4: Feature Redundancy</h2>
                </div>

                <div class="explanation">
                    <h3>What is Feature Redundancy?</h3>
                    <p>Feature redundancy identifies features that are highly correlated with each other. Redundant features provide little additional information and can cause problems in regression models.</p>

                    <details>
                        <summary>How does it work?</summary>
                        <div class="explanation-detail">
                            <p><strong>For each item separately:</strong></p>
                            <ol>
                                <li>Average all raters' ratings for Feature 1 (e.g., "animacy")</li>
                                <li>Average all raters' ratings for Feature 2 (e.g., "size")</li>
                                <li>Correlate these two vectors across all items (e.g., 60 items)</li>
                                <li>Repeat for all pairs of features</li>
                            </ol>
                            <p><strong>Example:</strong> If "living" and "animacy" both have high ratings for dogs/cats and low ratings for rocks/chairs, they'll have high positive correlation (redundant).</p>
                            <p><strong>High correlation (> 0.9) = redundant features</strong></p>
                            <p>Consider dropping one feature from highly redundant pairs.</p>
                        </div>
                    </details>

                    <div class="note">
                        <strong>Why does this matter?</strong>
                        <ul>
                            <li>Redundant features waste effort in rating</li>
                            <li>Can cause multicollinearity problems in regression</li>
                            <li>Make it harder to interpret which features are important</li>
                            <li>Goal: Keep diverse, independent features</li>
                        </ul>
                    </div>
                </div>

                <div id="redundancyAnalysis">
                    <button id="computeRedundancyBtn" class="btn-primary">Compute Feature Redundancy</button>

                    <div id="redundancyLoading" class="loading-message" style="display: none;">
                        <div class="spinner"></div>
                        <p>Computing feature-vs-feature correlations...</p>
                    </div>

                    <div id="redundancyResults" style="display: none;">
                        <div class="results-header">
                            <h3>Feature Redundancy Results</h3>
                        </div>

                        <!-- Threshold Control -->
                        <div class="threshold-control">
                            <label for="redundancyThreshold">Redundancy Threshold (show pairs above):</label>
                            <input type="range" id="redundancyThreshold" min="0" max="1" step="0.05" value="0.7">
                            <span id="redundancyThresholdValue">0.70</span>
                            <p class="explanation-text">Pairs with absolute correlation above this threshold are considered redundant.</p>
                        </div>

                        <!-- Redundant Pairs Summary -->
                        <div id="redundantPairsSummary" class="data-summary"></div>

                        <!-- Heatmap -->
                        <div class="heatmap-section">
                            <h4>Feature-vs-Feature Correlation Heatmap</h4>
                            <div class="results-header">
                                <p class="explanation-text">
                                    Shows correlation between each pair of features (averaged ratings across all items).
                                    Darker red = strong positive correlation. Darker blue = strong negative correlation.
                                </p>
                                <button id="downloadRedundancyHeatmap" class="btn-download">Download Heatmap (PNG)</button>
                            </div>
                            <div id="redundancyHeatmap" class="plotly-heatmap"></div>
                        </div>

                        <!-- Bar Chart of Top Redundant Pairs -->
                        <div class="chart-section">
                            <h4>Most Redundant Feature Pairs</h4>
                            <div class="results-header">
                                <p class="explanation-text">
                                    Feature pairs sorted by absolute correlation (highest first).
                                    Only showing pairs above the threshold.
                                </p>
                                <button id="downloadRedundancyChart" class="btn-download">Download Chart (PNG)</button>
                            </div>
                            <div class="chart-container">
                                <canvas id="redundancyChart"></canvas>
                            </div>
                        </div>

                        <details class="data-table-section">
                            <summary>View All Feature Pairs Data</summary>
                            <div id="redundancyTable"></div>
                        </details>

                        <div class="interpretation">
                            <h4>Interpretation Guide:</h4>
                            <ul>
                                <li><strong>|r| &gt; 0.9:</strong> Very high redundancy - strongly consider dropping one feature</li>
                                <li><strong>|r| = 0.7-0.9:</strong> Moderate redundancy - features are related but may provide different information</li>
                                <li><strong>|r| = 0.5-0.7:</strong> Low redundancy - features are somewhat related</li>
                                <li><strong>|r| &lt; 0.5:</strong> Independent features - both should be kept</li>
                                <li><strong>Positive correlation:</strong> Features move together (e.g., "living" and "animacy")</li>
                                <li><strong>Negative correlation:</strong> Features move oppositely (e.g., "natural" and "artificial")</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-navigation">
                    <button id="step4Prev" class="btn-prev">← Previous</button>
                    <button id="step4Next" class="btn-next" disabled>Next: Item Similarity →</button>
                </div>
            </div>
        </section>

        <section class="step-section" id="step5">
            <div class="card">
                <h2>Step 5: Item Similarity</h2>
                <p><em>Coming soon...</em></p>
                <div class="step-navigation">
                    <button id="step5Prev" class="btn-prev">← Previous</button>
                    <button id="step5Next" class="btn-next">Next →</button>
                </div>
            </div>
        </section>

        <section class="step-section" id="step6">
            <div class="card">
                <h2>Step 6: Make Decisions</h2>
                <p><em>Coming soon...</em></p>
                <div class="step-navigation">
                    <button id="step6Prev" class="btn-prev">← Previous</button>
                </div>
            </div>
        </section>
    </div>

    <!-- External Libraries -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.4.1/papaparse.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>

    <!-- Custom Libraries -->
    <script src="lib/dataframe.js"></script>
    <script src="lib/stats.js"></script>

    <!-- Main Script -->
    <script src="script.js"></script>
</body>
</html>
