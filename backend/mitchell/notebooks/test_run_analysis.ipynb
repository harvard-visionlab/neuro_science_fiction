{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Test run_analysis Handler Locally\n",
    "\n",
    "This notebook tests the run_analysis handler without S3 uploads.\n",
    "Files will be saved to `notebooks/data/test-results/` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from shared.brain_data import load_brain_data\n",
    "from shared.feature_data import load_feature_data\n",
    "from shared.analysis import doBrainAndFeaturePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up test parameters (same as what the Lambda handler would receive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Configuration:\n",
      "{\n",
      "  \"brain_subject\": 1,\n",
      "  \"year\": \"2020\",\n",
      "  \"group_name\": \"dopaminemachine\",\n",
      "  \"num_voxels\": 500,\n",
      "  \"zscore_braindata\": false,\n",
      "  \"testIndividualFeatures\": true,\n",
      "  \"analysis_id\": \"test-local-001\"\n",
      "}\n",
      "\n",
      "Output directory: data/test-results\n"
     ]
    }
   ],
   "source": [
    "# Test configuration (simulates Lambda event body)\n",
    "config = {\n",
    "    'brain_subject': 1,\n",
    "    'year': '2020',\n",
    "    'group_name': 'dopaminemachine',\n",
    "    'num_voxels': 500,\n",
    "    'zscore_braindata': False,\n",
    "    'testIndividualFeatures': True,  # Set to False for faster testing\n",
    "    'analysis_id': 'test-local-001'\n",
    "}\n",
    "\n",
    "# Local output directory (instead of S3)\n",
    "output_dir = 'data/test-results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Test Configuration:\")\n",
    "print(json.dumps(config, indent=2))\n",
    "print(f\"\\nOutput directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Test loading brain and feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading brain data for subject 1...\n",
      "Using cached brain data: /tmp/data-science-P1_converted.mat\n",
      "Loading feature data for 2020/dopaminemachine...\n",
      "Using cached feature data: /tmp/dopaminemachine_Ratings.csv\n",
      "Found 60 items, 16 features, 3 raters\n",
      "\n",
      "Brain data shape: (60, 21764, 6)\n",
      "Feature data shape: (60, 16)\n",
      "Number of features: 16\n",
      "Feature names: ['animaltype' 'clothing' 'container' 'edible' 'electronics' 'hardness'\n",
      " 'humanbodypart' 'living' 'moldable' 'moving' 'shape' 'size' 'taste'\n",
      " 'tool' 'transparency' 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading brain data for subject {config['brain_subject']}...\")\n",
    "brain_data = load_brain_data(config['brain_subject'])\n",
    "\n",
    "print(f\"Loading feature data for {config['year']}/{config['group_name']}...\")\n",
    "feature_data = load_feature_data(year=config['year'], group_name=config['group_name'])\n",
    "\n",
    "print(f\"\\nBrain data shape: {brain_data['D'].shape}\")\n",
    "print(f\"Feature data shape: {feature_data['R'].shape}\")\n",
    "print(f\"Number of features: {len(feature_data['featureNames'])}\")\n",
    "print(f\"Feature names: {feature_data['featureNames']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## Run Analysis\n",
    "\n",
    "Execute the full 1770-iteration analysis with progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "run-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis progress:   0%|                             | 1/1770 [00:00<00:05, 328.71it/s, iter/s=0.0, ETA=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis (1770 iterations)...\n",
      "\n",
      "ANALYZING SUBJECT NUMBER: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1770/1770 [06:37<00:00,  4.45it/s, iter/s=4.4, ETA=0.3m]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete!\n",
      "Elapsed time: 397.4s (6.62 min)\n",
      "Keys in results: dict_keys(['results', 'results_by_feature', 'all_betas'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Progress tracking\n",
    "pbar = tqdm(total=1770, desc=\"Analysis progress\")\n",
    "start_time = datetime.utcnow()\n",
    "\n",
    "def progress_callback(current, total):\n",
    "    pbar.update(1)\n",
    "    if current % 100 == 0 or current == total:\n",
    "        elapsed = (datetime.utcnow() - start_time).total_seconds()\n",
    "        rate = current / elapsed if elapsed > 0 else 0\n",
    "        pbar.set_postfix({\n",
    "            'iter/s': f'{rate:.1f}',\n",
    "            'ETA': f'{((total - current) / rate / 60):.1f}m' if rate > 0 else 'N/A'\n",
    "        })\n",
    "\n",
    "# Run analysis\n",
    "print(\"Starting analysis (1770 iterations)...\\n\")\n",
    "results = doBrainAndFeaturePrediction(\n",
    "    brain_data=brain_data,\n",
    "    feature_data=feature_data,\n",
    "    num_voxels=config['num_voxels'],\n",
    "    zscore_braindata=config['zscore_braindata'],\n",
    "    shuffle_features=False,\n",
    "    testIndividualFeatures=config['testIndividualFeatures'],\n",
    "    progress_callback=progress_callback\n",
    ")\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "end_time = datetime.utcnow()\n",
    "elapsed_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nAnalysis complete!\")\n",
    "print(f\"Elapsed time: {elapsed_time:.1f}s ({elapsed_time/60:.2f} min)\")\n",
    "print(f\"Keys in results: {results.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert-header",
   "metadata": {},
   "source": [
    "## Convert to DataFrames\n",
    "\n",
    "Test converting results to pandas DataFrames (same as Lambda handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convert-df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting results to DataFrames...\n",
      "\n",
      "Results DataFrame shape: (14160, 18)\n",
      "Columns: ['brain_subject', 'item1_idx', 'item2_idx', 'item1_name', 'item2_name', 'item1_cat', 'item2_cat', 'itemPair', 'same_category', 'r2_score', 'task', 'method', 'scoring', 'dist11', 'dist22', 'dist12', 'dist21', 'correct']\n",
      "\n",
      "Results by feature DataFrame shape: (56640, 20)\n",
      "Columns: ['feat_num', 'feat_name', 'brain_subject', 'item1_idx', 'item2_idx', 'item1_name', 'item2_name', 'item1_cat', 'item2_cat', 'itemPair', 'same_category', 'r2_score', 'task', 'method', 'scoring', 'dist11', 'dist22', 'dist12', 'dist21', 'correct']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain_subject</th>\n",
       "      <th>item1_idx</th>\n",
       "      <th>item2_idx</th>\n",
       "      <th>item1_name</th>\n",
       "      <th>item2_name</th>\n",
       "      <th>item1_cat</th>\n",
       "      <th>item2_cat</th>\n",
       "      <th>itemPair</th>\n",
       "      <th>same_category</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>task</th>\n",
       "      <th>method</th>\n",
       "      <th>scoring</th>\n",
       "      <th>dist11</th>\n",
       "      <th>dist22</th>\n",
       "      <th>dist12</th>\n",
       "      <th>dist21</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bear</td>\n",
       "      <td>cat</td>\n",
       "      <td>animal</td>\n",
       "      <td>animal</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544311</td>\n",
       "      <td>brain_prediction</td>\n",
       "      <td>encoding_model</td>\n",
       "      <td>individual</td>\n",
       "      <td>0.430449</td>\n",
       "      <td>0.517984</td>\n",
       "      <td>0.735480</td>\n",
       "      <td>0.500119</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bear</td>\n",
       "      <td>cat</td>\n",
       "      <td>animal</td>\n",
       "      <td>animal</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544311</td>\n",
       "      <td>brain_prediction</td>\n",
       "      <td>encoding_model</td>\n",
       "      <td>combo</td>\n",
       "      <td>0.430449</td>\n",
       "      <td>0.517984</td>\n",
       "      <td>0.735480</td>\n",
       "      <td>0.500119</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bear</td>\n",
       "      <td>cat</td>\n",
       "      <td>animal</td>\n",
       "      <td>animal</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544311</td>\n",
       "      <td>mind_reading</td>\n",
       "      <td>encoding_model</td>\n",
       "      <td>individual</td>\n",
       "      <td>0.420105</td>\n",
       "      <td>0.707555</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>1.036087</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bear</td>\n",
       "      <td>cat</td>\n",
       "      <td>animal</td>\n",
       "      <td>animal</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544311</td>\n",
       "      <td>mind_reading</td>\n",
       "      <td>encoding_model</td>\n",
       "      <td>combo</td>\n",
       "      <td>0.420105</td>\n",
       "      <td>0.707555</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>1.036087</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bear</td>\n",
       "      <td>cat</td>\n",
       "      <td>animal</td>\n",
       "      <td>animal</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544311</td>\n",
       "      <td>brain_prediction</td>\n",
       "      <td>botastic_templates</td>\n",
       "      <td>individual</td>\n",
       "      <td>0.412989</td>\n",
       "      <td>0.500988</td>\n",
       "      <td>0.616381</td>\n",
       "      <td>0.386240</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brain_subject  item1_idx  item2_idx item1_name item2_name item1_cat  \\\n",
       "0              1          0          1       bear        cat    animal   \n",
       "1              1          0          1       bear        cat    animal   \n",
       "2              1          0          1       bear        cat    animal   \n",
       "3              1          0          1       bear        cat    animal   \n",
       "4              1          0          1       bear        cat    animal   \n",
       "\n",
       "  item2_cat itemPair  same_category  r2_score              task  \\\n",
       "0    animal   (0, 1)              1  0.544311  brain_prediction   \n",
       "1    animal   (0, 1)              1  0.544311  brain_prediction   \n",
       "2    animal   (0, 1)              1  0.544311      mind_reading   \n",
       "3    animal   (0, 1)              1  0.544311      mind_reading   \n",
       "4    animal   (0, 1)              1  0.544311  brain_prediction   \n",
       "\n",
       "               method     scoring    dist11    dist22    dist12    dist21  \\\n",
       "0      encoding_model  individual  0.430449  0.517984  0.735480  0.500119   \n",
       "1      encoding_model       combo  0.430449  0.517984  0.735480  0.500119   \n",
       "2      encoding_model  individual  0.420105  0.707555  0.455400  1.036087   \n",
       "3      encoding_model       combo  0.420105  0.707555  0.455400  1.036087   \n",
       "4  botastic_templates  individual  0.412989  0.500988  0.616381  0.386240   \n",
       "\n",
       "   correct  \n",
       "0      0.5  \n",
       "1      1.0  \n",
       "2      1.0  \n",
       "3      1.0  \n",
       "4      0.5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Converting results to DataFrames...\")\n",
    "results_df = pd.DataFrame(results['results'])\n",
    "\n",
    "results_by_feature_df = None\n",
    "if results['results_by_feature']:\n",
    "    results_by_feature_df = pd.DataFrame(results['results_by_feature'])\n",
    "\n",
    "print(f\"\\nResults DataFrame shape: {results_df.shape}\")\n",
    "print(f\"Columns: {list(results_df.columns)}\")\n",
    "\n",
    "if results_by_feature_df is not None:\n",
    "    print(f\"\\nResults by feature DataFrame shape: {results_by_feature_df.shape}\")\n",
    "    print(f\"Columns: {list(results_by_feature_df.columns)}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nFirst few rows:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Files Locally\n",
    "\n",
    "Save all files to local directory (instead of S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "save-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files to data/test-results/...\n",
      "\n",
      "âœ“ Saved results.csv (2.55 MB)\n",
      "âœ“ Saved results_by_feature.csv (10.83 MB)\n",
      "âœ“ Saved all_betas.pth (168.32 MB)\n",
      "âœ“ Saved config.json (0.58 KB)\n",
      "\n",
      "All files saved to: /workspaces/neuro_science_fiction/backend/mitchell/notebooks/data/test-results/\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving files to {output_dir}/...\\n\")\n",
    "\n",
    "# Save CSVs\n",
    "results_csv_path = f'{output_dir}/results.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"âœ“ Saved results.csv ({os.path.getsize(results_csv_path) / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "if results_by_feature_df is not None:\n",
    "    results_by_feature_csv_path = f'{output_dir}/results_by_feature.csv'\n",
    "    results_by_feature_df.to_csv(results_by_feature_csv_path, index=False)\n",
    "    print(f\"âœ“ Saved results_by_feature.csv ({os.path.getsize(results_by_feature_csv_path) / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "# Save betas\n",
    "all_betas_path = f'{output_dir}/all_betas.pth'\n",
    "torch.save(results['all_betas'], all_betas_path)\n",
    "print(f\"âœ“ Saved all_betas.pth ({os.path.getsize(all_betas_path) / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "# Get actual number of iterations from results\n",
    "num_iterations = len(results['all_betas'])\n",
    "\n",
    "# Save config\n",
    "config_data = {\n",
    "    **config,\n",
    "    'timestamp': start_time.isoformat(),\n",
    "    'elapsed_time': elapsed_time,\n",
    "    'num_iterations': num_iterations,  # Use actual count instead of hardcoded 1770\n",
    "    'num_features': len(feature_data['featureNames']),\n",
    "    'feature_names': feature_data['featureNames'].tolist()\n",
    "}\n",
    "config_path = f'{output_dir}/config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_data, f, indent=2)\n",
    "print(f\"âœ“ Saved config.json ({os.path.getsize(config_path) / 1024:.2f} KB)\")\n",
    "\n",
    "print(f\"\\nAll files saved to: {os.path.abspath(output_dir)}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Compute Summary Statistics\n",
    "\n",
    "Test the summary statistics function (same as Lambda handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compute-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "{\n",
      "  \"num_iterations\": 1770,\n",
      "  \"elapsed_time\": 397.35,\n",
      "  \"elapsed_time_minutes\": 6.62,\n",
      "  \"brain_prediction_encoding_model_individual\": 0.8576,\n",
      "  \"brain_prediction_encoding_model_combo\": 0.9316,\n",
      "  \"brain_prediction_botastic_templates_individual\": 0.8768,\n",
      "  \"brain_prediction_botastic_templates_combo\": 0.9486,\n",
      "  \"mind_reading_encoding_model_individual\": 0.8189,\n",
      "  \"mind_reading_encoding_model_combo\": 0.9107,\n",
      "  \"mind_reading_botastic_templates_individual\": 0.8749,\n",
      "  \"mind_reading_botastic_templates_combo\": 0.9395,\n",
      "  \"same_category_accuracy\": 0.6667,\n",
      "  \"different_category_accuracy\": 0.9509,\n",
      "  \"num_same_category\": 120,\n",
      "  \"num_different_category\": 1650,\n",
      "  \"mean_r2_score\": 0.5495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Import the summary function from the handler\n",
    "from handlers.run_analysis import compute_summary_statistics\n",
    "\n",
    "# Get actual iteration count from results\n",
    "num_iterations = len(results['all_betas'])\n",
    "\n",
    "summary = compute_summary_statistics(results_df, elapsed_time, num_iterations)\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "## Verify Results\n",
    "\n",
    "Quick sanity checks on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Checks:\n",
      "\n",
      "âœ“ Expected 14160 rows, got 14160\n",
      "âœ“ same_category_accuracy: 0.6667 (valid range)\n",
      "âœ“ different_category_accuracy: 0.9509 (valid range)\n",
      "\n",
      "âœ“ Number of beta matrices: 1770\n",
      "âœ“ Beta matrix shape: (500, 16)\n",
      "\n",
      "ðŸŽ‰ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification Checks:\\n\")\n",
    "\n",
    "# Check number of rows\n",
    "expected_rows = 1770 * 8  # 1770 pairs Ã— 8 combinations (2 tasks Ã— 2 methods Ã— 2 scorings)\n",
    "actual_rows = len(results_df)\n",
    "print(f\"âœ“ Expected {expected_rows} rows, got {actual_rows}\" if actual_rows == expected_rows else f\"âœ— Expected {expected_rows} rows, got {actual_rows}\")\n",
    "\n",
    "# Check accuracy ranges\n",
    "for key, value in summary.items():\n",
    "    if 'accuracy' in key and value is not None:\n",
    "        if 0 <= value <= 1:\n",
    "            print(f\"âœ“ {key}: {value:.4f} (valid range)\")\n",
    "        else:\n",
    "            print(f\"âœ— {key}: {value:.4f} (out of range!)\")\n",
    "\n",
    "# Check betas\n",
    "print(f\"\\nâœ“ Number of beta matrices: {len(results['all_betas'])}\")\n",
    "if len(results['all_betas']) > 0 and results['all_betas'][0] is not None:\n",
    "    print(f\"âœ“ Beta matrix shape: {results['all_betas'][0].shape}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "## Explore Results\n",
    "\n",
    "Quick analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "explore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies by task/method/scoring:\n",
      "task              method              scoring   \n",
      "brain_prediction  botastic_templates  combo         0.9486\n",
      "                                      individual    0.8768\n",
      "                  encoding_model      combo         0.9316\n",
      "                                      individual    0.8576\n",
      "mind_reading      botastic_templates  combo         0.9395\n",
      "                                      individual    0.8749\n",
      "                  encoding_model      combo         0.9107\n",
      "                                      individual    0.8189\n",
      "Name: correct, dtype: float64\n",
      "\n",
      "============================================================\n",
      "Same category accuracy: 0.6667\n",
      "Different category accuracy: 0.9509\n"
     ]
    }
   ],
   "source": [
    "# Accuracies by task/method/scoring\n",
    "pivot = results_df.groupby(['task', 'method', 'scoring'])['correct'].mean().round(4)\n",
    "print(\"Accuracies by task/method/scoring:\")\n",
    "print(pivot)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Same vs different category\n",
    "brain_pred_subset = results_df[\n",
    "    (results_df['task'] == 'brain_prediction') &\n",
    "    (results_df['method'] == 'encoding_model') &\n",
    "    (results_df['scoring'] == 'combo')\n",
    "]\n",
    "same_cat_acc = brain_pred_subset[brain_pred_subset['same_category'] == 1]['correct'].mean()\n",
    "diff_cat_acc = brain_pred_subset[brain_pred_subset['same_category'] == 0]['correct'].mean()\n",
    "\n",
    "print(f\"Same category accuracy: {same_cat_acc:.4f}\")\n",
    "print(f\"Different category accuracy: {diff_cat_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9954e03-7067-4bba-a043-d2f318361ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dd615-0fe9-4d6f-af4d-ca796e1aeba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
