{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "boKuqQtks1cM"
      ],
      "authorship_tag": "ABX9TyPmL1oKdTKhjCEz+LDnigfy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/neuro_science_fiction/blob/main/2022/analyze_my_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze My Features\n",
        "\n",
        "The goal of this notebook is to analyze your features so you can identify which raters and features to use in your \"brain-prediction\" / \"mind-reading\" experiment."
      ],
      "metadata": {
        "id": "o_DYXCgMp5vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports (run me first)\n",
        "\n",
        "This section imports helpful Python libraries, and defines functions that analyze your ratings. Run every cell in this section, or collapse this section (arrows on the left), and run the whole section (see video walkthrough if this is unclear to you)."
      ],
      "metadata": {
        "id": "boKuqQtks1cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "QK7VACRk4MZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxeMp1Wup0n7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd \n",
        "from datetime import date\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from collections import defaultdict\n",
        "import seaborn as sns \n",
        "from functools import lru_cache\n",
        "from pdb import set_trace \n",
        "\n",
        "def download_ratings(team_name, dropRaters=[], dropFeatures=[]):\n",
        "  print(f\"==> Downloading data: {team_name}\")\n",
        "  team_name = team_name.lower()\n",
        "  filename = f\"{team_name}_Ratings.csv\"\n",
        "  url = f\"https://scorsese.wjh.harvard.edu/turk/experiments/nsf/survey/{team_name}/data\"\n",
        "  df = pd.read_csv(url)\n",
        "  df = df[~df.workerId.isin(dropRaters)]\n",
        "  df = df[~df.featureName.isin(dropFeatures)]\n",
        "\n",
        "  # drop rows for raters with incomplete datasets\n",
        "  # assuming max_count of ratings is expected/desired count\n",
        "  counts = df.groupby('workerId').rating.count()\n",
        "  max_count = counts.max()\n",
        "  dropRaters = counts[counts<max_count].index.values\n",
        "  if len(dropRaters) > 0:\n",
        "    print(f\"==> Dropping incomplete datasets: {dropRaters}\")\n",
        "    df = df[~df.workerId.isin(dropRaters)]\n",
        "\n",
        "  group_name = df.iloc[0].groupName\n",
        "  num_items = len(df.itemName.unique())\n",
        "  num_features = len(df.featureName.unique())\n",
        "  num_raters = len(df.workerId.unique())\n",
        "\n",
        "  print(\"=\"*50)\n",
        "  print(f\"FEATURE RATINGS: {group_name}\")\n",
        "  print(f\"{num_items} items, {num_features} features, {num_raters} raters\")\n",
        "  print(date.today().strftime(\"%B %d, %Y\"))\n",
        "  print(\"=\"*50)\n",
        "  \n",
        "  df.to_csv(filename, index=False)\n",
        "\n",
        "  items = sorted(df.itemName.unique())\n",
        "  features = sorted(df.featureName.unique())\n",
        "  raters = sorted(df.workerId.unique())\n",
        "  num_rows = len(df)\n",
        "  print(\"items:\", items)\n",
        "  print(\"features:\", features)\n",
        "  print(\"raters:\", raters)\n",
        "  expected_rows = len(items) * len(features) * len(raters)\n",
        "  #assert expected_rows == len(df), f\"Oops, expected {expected_rows}, got {len(df)}\"\n",
        "  #print(\"number of rows:\", num_rows)\n",
        "\n",
        "  return df \n",
        "\n",
        "def load_ratings(team_name):\n",
        "  team_name = team_name.lower()\n",
        "  filename = f\"{team_name}_Ratings.csv\"\n",
        "  df = pd.read_csv(filename)\n",
        "  group_name = df.iloc[0].groupName\n",
        "  num_items = len(df.itemName.unique())\n",
        "  num_features = len(df.featureName.unique())\n",
        "  num_raters = len(df.workerId.unique())\n",
        "\n",
        "  print(\"=\"*50)\n",
        "  print(f\"FEATURE RATINGS: {group_name}\")\n",
        "  print(f\"{num_items} items, {num_features} features, {num_raters} raters\")\n",
        "  print(date.today().strftime(\"%B %d, %Y\"))\n",
        "  print(\"=\"*50)\n",
        "\n",
        "  items = sorted(df.itemName.unique())\n",
        "  features = sorted(df.featureName.unique())\n",
        "  raters = sorted(df.workerId.unique())\n",
        "  num_rows = len(df)\n",
        "  print(\"items:\", items)\n",
        "  print(\"features:\", features)\n",
        "  print(\"raters:\", raters)\n",
        "  expected_rows = len(items) * len(features) * len(raters)\n",
        "  assert expected_rows == len(df), f\"Oops, expected {expected_rows}, got {len(df)}\"\n",
        "  print(\"number of rows:\", num_rows)\n",
        "\n",
        "  return df\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def compute_ratings_by_feature(team_name):\n",
        "  df = load_ratings(team_name)\n",
        "  items = sorted(df.itemName.unique())\n",
        "  features = sorted(df.featureName.unique())\n",
        "  raters = sorted(df.workerId.unique())\n",
        "\n",
        "  RatingsByFeature = {}\n",
        "  for featureName in features:\n",
        "    RatingsByFeature[featureName] = []\n",
        "    for rater in raters:\n",
        "      ratings = []\n",
        "      for itemName in items:\n",
        "        subset = df[(df.featureName==featureName) & (df.workerId==rater) & (df.itemName==itemName)]\n",
        "        assert len(subset)==1\n",
        "        ratings.append(subset.iloc[0].ratingScaled)\n",
        "      RatingsByFeature[featureName].append(np.array(ratings))\n",
        "    RatingsByFeature[featureName] = np.array(RatingsByFeature[featureName])\n",
        "\n",
        "  return df, RatingsByFeature\n",
        "\n",
        "def sort_features_by_consistency(RatingsByFeature):\n",
        "  results = defaultdict(list)\n",
        "  for feature_name in RatingsByFeature.keys():\n",
        "    rater_vs_rater = np.corrcoef(RatingsByFeature[feature_name])\n",
        "    num_raters = rater_vs_rater.shape[0]\n",
        "    avg_corr = np.nanmean(rater_vs_rater[np.triu_indices(num_raters,k=1)])\n",
        "    results['feature_name'].append(feature_name)\n",
        "    results['avg_corr'].append(avg_corr)\n",
        "  results = pd.DataFrame(results)\n",
        "  sorted = results.sort_values('avg_corr')\n",
        "  return sorted \n",
        "\n",
        "def rater_agreement(df, RatingsByFeature):\n",
        "  raters = sorted(df.workerId.unique())\n",
        "  results = defaultdict(list)\n",
        "  rater_vs_rater_all = []\n",
        "  for feature_name in RatingsByFeature.keys():\n",
        "    rater_vs_rater = np.corrcoef(RatingsByFeature[feature_name])\n",
        "    num_raters = rater_vs_rater.shape[0]\n",
        "    rater_consistency = (rater_vs_rater.sum(axis=1)-1)/(num_raters-1)\n",
        "    for rater_num,rater in enumerate(raters):\n",
        "      results['feature_name'].append(feature_name)\n",
        "      results['rater'].append(rater)\n",
        "      results['avg_corr'].append(rater_consistency[rater_num])\n",
        "    rater_vs_rater_all.append(rater_vs_rater)\n",
        "  rater_vs_rater_all = np.stack(rater_vs_rater_all)\n",
        "  results = pd.DataFrame(results)\n",
        "\n",
        "  return results, rater_vs_rater_all\n",
        "\n",
        "def plot_sorted_features(sorted_features):\n",
        "  sns.set(rc={'figure.figsize':(14, len(sorted_features)*.33)})\n",
        "  ax = sns.barplot(data=sorted_features, y=\"feature_name\", x=\"avg_corr\", orient=\"h\")\n",
        "  ax.set_ylabel(\"Feature Name\")\n",
        "  ax.set_xlabel(\"Average Correlation Between Raters (across items)\");\n",
        "  ax.set_xlim([0,1]);\n",
        "  return ax \n",
        "\n",
        "def compute_feature_reliability(team_name):  \n",
        "  df, RatingsByFeature = compute_ratings_by_feature(team_name)\n",
        "  sorted_features = sort_features_by_consistency(RatingsByFeature)\n",
        "\n",
        "  print(\"=\"*50)\n",
        "  print(f\"FEATURE RELIABILTY:\")\n",
        "  print(\"How consistent were the feature ratings between raters?\")\n",
        "  print(\"Consider two raters, and their 60 ratings (1 per item).\")\n",
        "  print(\"Correlate those ratings, then repeat for all pairs of\")\n",
        "  print(\"raters, and compute the average rating. Higher numbers\")\n",
        "  print(\"indicate higher consistency in ratings.\")\n",
        "  print(\"=\"*50)\n",
        "  print(sorted_features)\n",
        "  print(\"\\n\")\n",
        "  ax = plot_sorted_features(sorted_features)\n",
        "  return ax\n",
        "\n",
        "def compute_rater_agreement(team_name):\n",
        "  df, RatingsByFeature = compute_ratings_by_feature(team_name)\n",
        "  raters = sorted(df.workerId.unique())\n",
        "  agreement, rater_vs_rater = rater_agreement(df, RatingsByFeature)\n",
        "\n",
        "  print(\"=\"*50)\n",
        "  print(f\"RATER AGREEMENT:\")\n",
        "  print(\"How well did each rater correlate with the other \")\n",
        "  print(\"raters, on average?\")\n",
        "  print(\"=\"*50)\n",
        "  print(agreement.groupby('rater').mean())\n",
        "\n",
        "  ax = sns.heatmap(np.nanmean(rater_vs_rater, axis=0), square=True,\n",
        "                   xticklabels=raters, yticklabels=raters,\n",
        "                   vmin=0, vmax=1)\n",
        "  return ax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_feature_vs_feature_corr(df):\n",
        "  features = sorted(df.featureName.unique())\n",
        "  items = sorted(df.itemName.unique())\n",
        "  M = []\n",
        "  for feature in features:\n",
        "    item_ratings = []\n",
        "    for item in items:\n",
        "      subset = df[(df.featureName==feature) & (df.itemName==item)]\n",
        "      item_ratings.append(subset.ratingScaled.mean())\n",
        "    M.append(item_ratings)\n",
        "  M = np.array(M)\n",
        "  feature_vs_feature = np.corrcoef(M)\n",
        "\n",
        "  return M, feature_vs_feature\n",
        "\n",
        "def feature_redundancy(df, feature_vs_feature):\n",
        "  features = sorted(df.featureName.unique())\n",
        "  items = sorted(df.itemName.unique())\n",
        "  corrs = defaultdict(list)\n",
        "  num_features = len(features)\n",
        "  for feature1 in range(0,num_features-1):\n",
        "    for feature2 in range(feature1+1, num_features):\n",
        "      corr = feature_vs_feature[feature1,feature2]\n",
        "      featureName1 = features[feature1]\n",
        "      featureName2 = features[feature2]\n",
        "      pair = f\"{featureName1}_{featureName2}\"\n",
        "      corrs['item1'].append(featureName1)\n",
        "      corrs['item2'].append(featureName2)\n",
        "      corrs['pair'].append(pair)\n",
        "      corrs['correlation'].append(corr)\n",
        "      corrs['abs_correlation'].append(np.abs(corr))\n",
        "      corrs['sign'].append(\"positive\" if corr>0 else \"negative\")\n",
        "  corrs = pd.DataFrame(corrs)\n",
        "  corrs = corrs.sort_values('abs_correlation')\n",
        "  \n",
        "  return corrs\n",
        "\n",
        "def plot_feature_correlation_heatmap(features, feature_vs_feature):\n",
        "  print(\"=\"*50)\n",
        "  print(f\"FEATURE REDUNDANCY:\")\n",
        "  print(\"How correlated are our features?\")\n",
        "  print(\"Here were plotting the abs(correlation) between\")\n",
        "  print(\"every pair of features. Lower correlations are\")\n",
        "  print(\"preferred because it means the features carry\")\n",
        "  print(\"independent information.\")\n",
        "  print(\"=\"*50)\n",
        "\n",
        "  sns.set(rc={'figure.figsize':(8,8)})\n",
        "  ax = sns.heatmap(np.abs(feature_vs_feature), square=True,\n",
        "                   xticklabels=features, yticklabels=features,\n",
        "                   vmin=0, vmax=1);  \n",
        "  plt.show()\n",
        "\n",
        "def plot_feature_correlation_bars(features, corrs, threshold=.9):\n",
        "  print(\"=\"*50)\n",
        "  print(f\"Now as bars, sorted from lowest to highest abs(correlation).\")\n",
        "  print(\"=\"*50)\n",
        "\n",
        "  num_features = len(features)\n",
        "  sns.set(rc={'figure.figsize':(14,num_features*3)})\n",
        "  ax = sns.barplot(data=corrs, y=\"pair\", x=\"abs_correlation\", orient=\"h\")\n",
        "  ax.set_ylabel(\"Feature Name\")\n",
        "  ax.set_xlabel(\"Correlation Across Items\");\n",
        "  ax.set_xlim([0,1]);\n",
        "  ax.axvline(threshold, color='gray', linestyle='--');\n",
        "  plt.show()  \n",
        "\n",
        "def compute_feature_redundancy(team_name, threshold = .90):\n",
        "  df = load_ratings(team_name)\n",
        "  features = sorted(df.featureName.unique())\n",
        "  \n",
        "  M, feature_vs_feature = compute_feature_vs_feature_corr(df)\n",
        "  corrs = feature_redundancy(df, feature_vs_feature)\n",
        "\n",
        "  plot_feature_correlation_heatmap(features, feature_vs_feature)\n",
        "  \n",
        "  plot_feature_correlation_bars(features, corrs, threshold=threshold)\n",
        "  "
      ],
      "metadata": {
        "id": "VKaWy0Grp7On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_item_vs_item_corr(df):\n",
        "  features = sorted(df.featureName.unique())\n",
        "  items = sorted(df.itemName.unique())\n",
        "  M = []\n",
        "  for feature in features:\n",
        "    item_ratings = []\n",
        "    for item in items:\n",
        "      subset = df[(df.featureName==feature) & (df.itemName==item)]\n",
        "      item_ratings.append(subset.ratingScaled.mean())\n",
        "    M.append(item_ratings)\n",
        "  M = np.transpose(np.array(M))\n",
        "  item_vs_item = np.corrcoef(M)\n",
        "\n",
        "  return M, item_vs_item\n",
        "\n",
        "\n",
        "def plot_item_correlation_heatmap(items, item_vs_item):\n",
        "  print(\"=\"*50)\n",
        "  print(f\"ITEM SIMILARITY:\")\n",
        "  print(\"=\"*50)\n",
        "\n",
        "  sns.set(rc={'figure.figsize':(14,14)})\n",
        "  ax = sns.heatmap(np.abs(item_vs_item), square=True,\n",
        "                   xticklabels=items, yticklabels=items,\n",
        "                   vmin=-1, vmax=1);  \n",
        "  plt.show()\n",
        "\n",
        "def compute_item_similarity(team_name, threshold = .80):\n",
        "  df = load_ratings(team_name)\n",
        "  features = sorted(df.featureName.unique())\n",
        "  items = sorted(df.itemName.unique())\n",
        "\n",
        "  M, item_vs_item = compute_item_vs_item_corr(df)\n",
        "  \n",
        "  num_items = item_vs_item.shape[0]\n",
        "  upper_diag = item_vs_item[np.triu_indices(num_items,k=1)]\n",
        "  total_pairs = upper_diag.shape[0]\n",
        "  mean_corr = upper_diag.mean()\n",
        "  number_similar = (upper_diag > threshold).sum()\n",
        "  percent_similar = number_similar/total_pairs * 100\n",
        "\n",
        "  plot_item_correlation_heatmap(items, item_vs_item)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(f\"Total Pairs of Items: {total_pairs}\")\n",
        "  print(f\"Average correlation across pairs (lower is better): {mean_corr:4.2f}\")\n",
        "  print(f\"Number of similar pairs (r > {threshold}): {number_similar}\")\n",
        "  print(f\"Percent similar: {percent_similar:3.2f}\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(f\"So about {percent_similar:3.2f} percent of the time your model is \")  \n",
        "  print(\"likely to fail, not because it isn't a good set of features, but\")  \n",
        "  print(\"because these items are so similar to each other on these features.\")  \n"
      ],
      "metadata": {
        "id": "BWE_o1w6wp5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_most_similar_item(team_name, N=1):\n",
        "  df = load_ratings(team_name)\n",
        "  features = sorted(df.featureName.unique())\n",
        "  items = sorted(df.itemName.unique())\n",
        "\n",
        "  M, item_vs_item = compute_item_vs_item_corr(df)\n",
        "\n",
        "  results = defaultdict(list)\n",
        "  for item_idx in range(len(items)):\n",
        "    corrs = item_vs_item[item_idx]\n",
        "    corrs[item_idx] = -np.inf\n",
        "    most_similar_idx = corrs.argsort()[-N]\n",
        "    itemName = items[item_idx]\n",
        "    mostSimilarItem = items[most_similar_idx]\n",
        "    corr = corrs[most_similar_idx]\n",
        "    results['itemName'].append(itemName)\n",
        "    results['mostSimilarItem'].append(mostSimilarItem)\n",
        "    results['correlation'].append(corr)\n",
        "  results = pd.DataFrame(results)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"=\"*50)\n",
        "  print(f\"MOST SIMILAR ITEM (N={N}):\")\n",
        "  print(\"=\"*50)\n",
        "  return results"
      ],
      "metadata": {
        "id": "DrYKD8KE3FEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Your Dataset\n",
        "\n",
        "Let's make sure your dataset is available and that we can download it. This cell will fetch your data from the server, and store a local copy. It gets output as a \"dataframe\", and the total length of this dataframe should equal: numItems * numFeatures * numRaters. For example, if you have 16 features, and 3 raters, the length should be `60*16*3=2880`"
      ],
      "metadata": {
        "id": "20VziyYusyUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "team_name = 'dopaminemachine'\n",
        "\n",
        "dropRaters = []\n",
        "dropFeatures = []\n",
        "df = download_ratings(team_name, dropRaters=dropRaters, dropFeatures=dropFeatures)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2Xq6wOAWrXjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE RELIABILITY\n",
        "\n",
        "How consistently did people rate these features?\n",
        "We want to use features that people showed pretty high agreement on.\n",
        "\n",
        "Consider the extreme, where people respond randomly. Of course you would not expect your feature to predict brain data (because it's a bunch of random numbers!).\n",
        "\n",
        "We can test for agreement, or reliability in our feature estimates by correlating the ratings for each subject with every other subject. \n",
        "\n",
        "The average of these pairwise correlations tells us how much agreement there was between our subjects (on average).\n",
        "\n",
        "Features are sorted from least to most reliable (higher values are better)\n",
        "\n",
        "(reliability = average correlation across pairs of subjects)"
      ],
      "metadata": {
        "id": "6YKFUO42p3Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_feature_reliability(team_name);"
      ],
      "metadata": {
        "id": "xauUQU0x7xvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_rater_agreement(team_name);"
      ],
      "metadata": {
        "id": "F_cx8VHh9biS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Redundancy\n",
        "\n",
        "Next, we correlate the ratings from each feature with each other feature.\n",
        "\n",
        "We want to know whether each feature is providing potentially useful information.\n",
        "\n",
        "Highly correlated features are redundant (given one feature, the other feature doesn't tell you anything new or useful).\n",
        "\n",
        "The same is true for highly negatively correlated features, so here we're sorting by absolute value of the correlation, from lowest to highest. Higher abs(correlation) values are worse (more redundancy).\n",
        "\n",
        "How high is too high? Greater than .9 could be problematic, because it could make it difficult for the regression algorithm to find an appropriate \"weight\" for each of your features."
      ],
      "metadata": {
        "id": "7noReoATCnBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_feature_redundancy(team_name)"
      ],
      "metadata": {
        "id": "LrSR1FMICGP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Discriminability\n",
        "\n",
        "How different are the 60 items on your features?\n",
        "Even if your model is perfect (i.e., these are the exact features the brain cares about), if all of your items share the same features, your model will fail. \n",
        "\n",
        "Let's flesh out why:\n",
        "\n",
        "Remember that we use mitchell's scoring method. He holds out two items, and then trains the model on the remaining 58. Then he uses the model to predict activity to the 2 held-out items.  \n",
        "\n",
        "The prediction is scored as correct, if the predicted brain pattern is more like the true brain pattern for that item, than for the brain pattern for the other held-out-item.\n",
        "\n",
        "But if the features are identical for two items, the model will predict exactly the same brain response to each object! So basically it will fail every time. \n",
        "\n",
        "This doesn't mean your model is wrong, it means you have the wrong items to test your particular model. So we want features that we think are useful not just in general, but for telling these particular items apart. \n",
        "\n",
        "Ideally, our features could be used to tell all possible items apart!\n"
      ],
      "metadata": {
        "id": "jDIKejjVDBhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_item_similarity(team_name)"
      ],
      "metadata": {
        "id": "UCg1rYdAv-Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_most_similar_item(team_name, N=1)"
      ],
      "metadata": {
        "id": "itz7kWeExtRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now What?\n",
        "\n",
        "Next, send an e-mail to alvarez@wjh.harvard.edu to let me know:\n",
        "- 1) Whether you want to drop any raters\n",
        "- 2) Whether you want to drop any features\n",
        "- 3) Whether you want to add any features (and if so, include your feature survey questions for your new features)\n",
        "\n",
        "I need your final features at least a 2 days before our next meeting, so that I have time to run the brain-prediction & mind-reading analyses ahead of time (it's too slow for us to do in a 1 hour meeting, so I'll compute the feature-to-voxel weights in advance for you)."
      ],
      "metadata": {
        "id": "Z3VECKXl40pd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohv23H4A5iXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}